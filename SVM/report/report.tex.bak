\documentclass[conference,compsoc]{IEEEtran}
\usepackage{CJK}
\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi
\ifCLASSINFOpdf
\else
\fi
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{booktabs}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor}
\begin{document}
\title{Support Vector Machine Report}
\author{\IEEEauthorblockN{Peijun Ruan  11610214}\\
\IEEEauthorblockA{School of Computer Science and Engineering\\
Southern University of Science and Technology\\
Email: 11610214@mail.sustc.edu.cn}}
\maketitle
\IEEEpeerreviewmaketitle
\section{Preliminaries}
\subsection{Introduction}
In machine learning,support vector machines(\textbf{SVM}) are supervised learning models that using algorithm to analyze data used for classification and regression analysis.Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other.[1]
\subsection{Application}
SVM can be used to solve various real world problems:
\begin{itemize}
    \item SVM can be used in text and hypertext categorization.
    \item Hand-written characters can be recognized using SVM. \\
    etc.
\end{itemize}

In this project, we will be given a train set and a test set and a time limit, our goal is to train a model in limited time and print the classification result of test set.
\section{Methodology}
\subsection{Model Design}
There are many method to train a support vector machine model.One algorithm is Pegasos which using the idea of sub gradient descent, another one is Sequential minimal optimization(SMO).In this project, I implement both of these two algorithm.

For Pegasos,each loop we randomly choose a sample for gradient descent,and for certain number of loops,we get the final result[3].

For SMO,each loop we choose a pair of alpha and update them until the iteration times larger than setting value or the model convergence[2].
\subsection{Representation}

\subsubsection{Notation}
\begin{itemize}
    \item Pegasos
    \begin{itemize}
        \item S: the data set
        \item $\lambda$: the parameter for gradient calculation
        %\item T: iteration times
    \end{itemize}
    \item SMO
    \begin{itemize}
        \item $\alpha$: the vector of lagrangian multiplier
        \item T: iteration times
        \item error: the list of error of all training samples
        \item kernel(K): the kernel trick used in algorithm
        \item conv: the convergence judgment
    \end{itemize}
\end{itemize}
\subsubsection{Data Structure}
All the data will first store in list and then transfer into numpy array for further calculation convenience.All the vectors during the algorithm training are also numpy.array type.
\subsection{Functions}
Here show the main functions I implement in this project.
\begin{itemize}
    \item SMO: the implementation of SMO algorithm
    \item Pegasos: the implementation of Pegasos algorithm
    \item predict: the function that predicts result for given data set
    \item call\_e: calculate the error of a single sample
    \item kernel: the kernel function that map (x,y) to $\phi$(x,y)
    \item load\_data: loading data from the given path.
    \item cal\_L\_H: calculate the up and low border of parameter
    \item call\_w: calculate the w vector for model
\end{itemize}


\subsection{Detail of Algorithm}
Here shows the psudocode of SMO and Pegasos.

\begin{algorithm*}
    \caption{SMO}
    \begin{algorithmic}
        \Require data set $S$,data labels $L$,kernel $K$,iteration times $T$,convergence judgment $E$
        \Ensure $w,b$
        \State $\alpha \gets$zero vector
        \State $itercount,b \gets 0$
        \While{itercount $<$ T}
            \State $itercount\gets itercount+1$
            \State $\alpha_{temp} \gets \alpha$
            \For{i in range(S.shape[0])}
                \State $error_{i}$ $\gets$ call\_e(i)
                \If{sample i does not obey KKT condition}
                    \State j$\gets$ randint(i,S.shape[0])
                    \State $error_{j}$ $\gets$ call\_e(j)
                    \State $\alpha_{i},\alpha_{j} \gets \alpha[i],\alpha[j]$
                    \State L,H $\gets$ cal\_L\_H
                    \If{L==H}
                        \State continue
                    \EndIf
                    \State $\theta \gets$ 2*K(i,j)-K(i,i)-K(j,j)
                    \State $\alpha[j]-=L[j]*(error_{i}-error_{j})/\theta$
                    \If{$\alpha[j]>H$}
                        \State $\alpha[j]\gets H$
                    \ElsIf{$\alpha[j]<L$}
                        \State $\alpha[j]\gets L$
                    \EndIf
                    \State $\alpha[i]+=L[i]*L[j]*(\alpha_{j}-\alpha[j])$
                    \State update $b$
                \EndIf
            \EndFor
            \If{$\alpha-\alpha_{temp}<E$}
                \State break
            \EndIf
        \EndWhile
        \State $w\gets$ call\_w
        \Return $w,b$
    \end{algorithmic}
\end{algorithm*}

\begin{algorithm*}
    \caption{Pegasos}
    \begin{algorithmic}
        \Require data set $S$,data labels $L$,$\lambda$%,iteration times $T$
        \Ensure $w,b$
        \State $t \gets 0$
        \State $w \gets zero \ vector$
        \For{i in range(100)}
            \For{j in range($S$.shape[0])}
                \State $t \gets t+1$
                \State $\theta \gets$ 1/($\lambda$*t)
                \State p $\gets$ w*S[j]
                \If{L[j]*p<1}
                    w$\gets$w-$\theta$*($\lambda$*w-L[j]*S[j])
                \Else
                    w$\gets$w-$\theta$*$\lambda$*w
                \EndIf
            \EndFor
        \EndFor
        \State b$\gets$calculate b
        \Return $w,b$
    \end{algorithmic}
\end{algorithm*}
\section{Empirical Verification}

\subsection{Design}
Since I implement two algorithms to train the SVM model, we can easily compare the accuracy of the final result and the training time between two models.

Here we just simply define the accuracy as the number of correct marks over the number of all test samples.
\subsection{Data}
We are only given one data set but we need a train set and a test set,so we can divide the total set into proper two part,one for training and the other for testing.
I choose 90\% of data for training and rest for testing.

\subsection{Hyperparameters}
There are many hyperparameters you need to pay attention to.Here show the strategy I choose the value of them.
\begin{itemize}
    \item SMO:
        \begin{itemize}
            \item $C:$the tolerance of soft margin, default value is 1.0
            \item $Kernel:$the kernel trick used in calculation, default using RBF kernel
            \item $error:$the lower border of error that exit the loop, default value is 0.001
            \item $\sigma:$the radius length of RBF kernel, default value is 1
            \item $degree:$the degree of polynomial kernel, default value is 2(quadratic kernel)
            \item $T:$the maximum iteration times to finish training, default value is 1000
        \end{itemize}
    \item Pegasos:
        \begin{itemize}
            \item $\lambda:$the parameter for gradient calculation. default value is 2
        \end{itemize}
\end{itemize}
\subsection{Test Environment}
CPU:i5-7300HQ  2.50GHz

RAM:8.0GB

Number of Process: only one process
\subsection{Performance}
To compare the performance, we just compare the accuracy and training time.I use the default value for all the hyperparameters at first and then change the hyperparameters to check the changes.

\subsubsection{Default} \quad \\
For training time cost: Pegasos costs 3.52s and SMO costs 2.34s

For accuracy:Pegasos 100\%  SMO 100\%
\subsubsection{Hyperparameters changes} \quad \\

\subsection{Analysis}
From the simple compare we find that the speed of SMO is faster than Pegasos,but from the principle of these two algorithm we know it is wired.If I change the $C$ to 5,the time cost of SMO become larger than Pegasos,and that's because $C$ determine how many error samples model can tolerate during training, the bigger $C$ is, the more samples SMO has to deal with.

Besides, the accuracy of SMO and Pegasos are 1 is also strange,I think that's because the test data set is small.If we enlarge the size of test data set, the accuracy result will be more realistic.

However,even Pegasos has a faster training speed, I did not add kernel trick for it, it can not deal with some complex data sets which is not a linear classification set well.











\bibliographystyle{IEEEtran}

\begin{thebibliography}{1}
\bibitem{reference}
Support vector machine. (2018). Retrieved from https://en.wikipedia.org/wiki/Support\_vector\_machine

\bibitem{reference}
\begin{CJK*}{GBK}{song}
序列最小优化算法（SMO）浅析. (2018). Retrieved from https://www.jianshu.com/p/eef51f939ace
\end{CJK*}

\bibitem{reference}
\begin{CJK*}{GBK}{song}
svm随机次梯度下降算法-pegasos - shiqi,bao的博客 - CSDN博客. (2018). Retrieved from https://blog.csdn.net/sinat\_27612639/article/details/70037499
\end{CJK*}

%\bibitem{reference}


\end{thebibliography}


\end{document}


